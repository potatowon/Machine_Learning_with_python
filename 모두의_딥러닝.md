

# 01 ë¨¸ì‹ ëŸ¬ë‹ì˜ ê°œìš”
 - ğŸ’¡ supervised : learning with labeled examples (training set) `ì£¼ë¡œ ë°°ìš¸ë‚´ìš©`
<p align='center'><img src="https://user-images.githubusercontent.com/118495946/214555562-b7df5d97-b9e8-471a-9989-0a9dc50ae727.jpeg" width="600" height="300"/></p>
 -  ğŸ’¡ Unsupervised learning : un-labeled data
  - Google news grouping
  - Word clustering

## 01-1 types of supervised learning
1. ğŸ’¡ `regression` : prediction final exam score based on time spent (ì—°ì†ì ì¸ ê°’)
2. ğŸ’¡ `binart calssification` : pass/non-pass based on time spent ( 2ê°œì˜ ê°’)
3. ğŸ’¡ `multi-label classification` : Letter grade based on time spent ( ë¶ˆì—°ì†ì ì¸ ì—¬ëŸ¬ê°œì˜ ê°’)

# 02 ì„ í˜• íšŒê·€
ğŸ’¡ ëŒ€í‘œì ì¸ ì˜ˆì‹œë¡œ : prediction final exam score based on time spent ê°€ ìˆë‹¤ê³  í•˜ì <\br>
ê° `training dataset` ì„ ì²´í¬í•˜ê³  ì´ì™€ ê°€ì¥ ì˜ ë§ëŠ” ì„ í˜•ì‹ì„ ì°¾ëŠ” ê²ƒì´ ë¬¸ì œì´ë‹¤

## 02-1 (Linear) Hypothesis
### ìˆ˜ì‹
$$H(x) = Wx + b (W : ê°€ì¤‘ì¹˜, b: bias)$$
ê°€ì¤‘ì¹˜ì™€ ì ˆí¸ì„ ì¡°ì ˆí•˜ì—¬ ì ì ˆí•œ ì„ í˜• ë°©ì •ì‹ì„ ì°¾ëŠ”ë‹¤.

## 02-02 Cost function(1) : square
 ğŸ’¡ ëª¨ë¸ê³¼ ì‹¤ì œ ë°ì´í„° ê°’ì˜ ì°¨ì´
 - How fit the line to our (training) data
 - ì œê³±
$$(H(x) - y )^{2} = loss function $$
ì´ë•Œ $y$ ëŠ” ì˜ˆì¸¡ ê°’
### ìˆ˜ì‹
$$cost = {1 \over m} \Sigma_{i=1}^m (H(x^{(i)} - y^{(i)})^2$$


# 03 How to minimize cost
- cost functionì˜ í˜•íƒœë¥¼ ë³´ë©´ -> $W$ ì— ëŒ€í•œ 2ì°¨ ë°©ì •ì‹ì˜ í˜•íƒœì´ë‹¤. -> ë”°ë¼ì„œ ê°’ì´ costì˜ ê°’ì´ ìµœì†Œê°€ ë˜ëŠ” $W$ ê°’ì„ ì°¾ì•„ì•¼ í•œë‹¤.
- ğŸ”¥ ìµœì†Œê°’ì„ ì°¾ëŠ” ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì´ ì¡´ì¬ í•œë‹¤. ì´ì¤‘ì—ì„œ ìš°ë¦¬ëŠ” `ê²½ì‚¬í•˜ê°•ë²•(gradiednt descent algorithm)` ì— ëŒ€í•´ í•™ìŠµí•œë‹¤. 

## 03-1 ê²½ì‚¬í•˜ê°•ë²•
- ì‘ë™ê¸°ì‘
1. start with initial guesses
 - start at 0,0 (or any other value)
 - Keeping changin **W** and **b** a little bit to try and reduce $cost(W, b)$
2. $cost(W, b)$ê°€ ê°ì†Œí•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ê° íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•œë‹¤.
3. ë”°ë¼ì„œ ìˆ˜ë ´í•˜ëŠ” `local minimum`ì„ ì°¾ëŠ”ë‹¤.

### ìˆ˜ì‹
$$W := W - \alpha {\partial \over \partial{W} } cost(W)$$
$\alpha$ : learning rate


# 05 Binary classification
ğŸ’¡ ëŒ€í‘œì  : ìŠ¤íŒ¸/ í˜ì´ìŠ¤ë¶ì˜ show or hide/ ì‹ ìš©ì¹´ë“œ ì´ìƒ ê±°ë˜ ê°ì§€
```
ì‹œê°„ì— ëŒ€í•œ íŒ¨ë…¼íŒ¨ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤ê³  ê°€ì •í•´ë³´ì
ìì—°ìŠ¤ëŸ½ê²Œ... ìƒê°í•˜ë©´ ê³µë¶€ì‹œê°„ì´ ëŠ˜ ìˆ˜ë¡ íŒ¨ìŠ¤ì¼ í™•ë¥ ì´ ë†’ë‹¤.
ë‹¤ë§Œ, ì´ì— ëŒ€í•œ H ë¥¼ ì„ í˜•ë°©ì •ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ê²Œ ë˜ë©´ pass = 1 ì¸ë° ê·¸ ì˜ˆì¸¡ê°’ì´ ë„ˆë¬´ ë†’ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ë¬¸ì œê°€ ì¡´ì¬í•œë‹¤.
ê·¸ë ‡ë‹¤ê³  ê¸°ìš¸ê¸°ë¥¼ ë‚®ì¶”ë©´ pass/ non passì˜ ì„ê³„ì ì´ ë‹¬ë¼ì ¸ ë¬¸ì œê°€ ëœë‹¤.

-> ë”°ë¼ì„œ ì´ëŸ¬í•œ ì„ í˜•í•¨ìˆ˜ë¥¼ ë¹„ì„ í˜•í•¨ìˆ˜ë¡œ ë§Œë“¤ì–´ ì‘ë™í•˜ê²Œ í•˜ëŠ” ê¸°ì‘ì´ activate function ì´ë‹¤
```

## 05-1 activate function(1) : Sigmoid
- We know Y is `0` or `1` 
 - $H(x) = Wx + b$
- Hypothesis can give calues large than 1 or less than 0

### ìˆ˜ì‹
$$z = Wx + b$$
$$H(x) = g(z)$$
$$g(z) = {1 \over {1+e^{-z}}}$$

## 05-2 ìƒˆë¡œìš´ cost function
$$cost(W) = {1 \over m} \Sigma c(H(x), y)$$
$$c(H(x), y) = \begin{cases}
-log(H(x)), & :y=1 \\
-log(1-H(x)), & :y=0
\end{cases}$$
