- [01 ë¨¸ì‹ ëŸ¬ë‹ì˜ ê°œìš”](#01---------)
  * [01-1 types of supervised learning](#01-1-types-of-supervised-learning)
- [02 ì„ í˜• íšŒê·€](#02------)
  * [02-1 (Linear) Hypothesis](#02-1--linear--hypothesis)
    + [ìˆ˜ì‹](#--)
  * [02-02 Cost function(1) : square](#02-02-cost-function-1----square)
    + [ìˆ˜ì‹](#---1)
- [03 How to minimize cost](#03-how-to-minimize-cost)
  * [03-1 ê²½ì‚¬í•˜ê°•ë²•](#03-1------)
    + [ìˆ˜ì‹](#---2)
- [05 Binary classification](#05-binary-classification)
  * [05-1 activate function(1) : Sigmoid](#05-1-activate-function-1----sigmoid)
    + [ìˆ˜ì‹](#---3)
  * [05-2 ìƒˆë¡œìš´ cost function](#05-2-----cost-function)
- [06 Softmax classification: Multinomial classification](#06-softmax-classification--multinomial-classification)
  * [06-1 activate funciton(2) Softmax](#06-1-activate-funciton-2--softmax)
    + [ìˆ˜ì‹](#---4)
  * [06-2 cost function(3) - cross-entropy cost function](#06-2-cost-function-3----cross-entropy-cost-function)
    + [ìˆ˜ì‹](#---5)

# 01 ë¨¸ì‹ ëŸ¬ë‹ì˜ ê°œìš”
 - ğŸ’¡ supervised : learning with labeled examples (training set) `ì£¼ë¡œ ë°°ìš¸ë‚´ìš©`
<p align='center'><img src="https://user-images.githubusercontent.com/118495946/214555562-b7df5d97-b9e8-471a-9989-0a9dc50ae727.jpeg" width="600" height="300"/></p>
 -  ğŸ’¡ Unsupervised learning : un-labeled data
  - Google news grouping
  - Word clustering

## 01-1 types of supervised learning
1. ğŸ’¡ `regression` : prediction final exam score based on time spent (ì—°ì†ì ì¸ ê°’)
2. ğŸ’¡ `binart calssification` : pass/non-pass based on time spent ( 2ê°œì˜ ê°’)
3. ğŸ’¡ `multi-label classification` : Letter grade based on time spent ( ë¶ˆì—°ì†ì ì¸ ì—¬ëŸ¬ê°œì˜ ê°’)

# 02 ì„ í˜• íšŒê·€
ğŸ’¡ ëŒ€í‘œì ì¸ ì˜ˆì‹œë¡œ : prediction final exam score based on time spent ê°€ ìˆë‹¤ê³  í•˜ì <\br>
ê° `training dataset` ì„ ì²´í¬í•˜ê³  ì´ì™€ ê°€ì¥ ì˜ ë§ëŠ” ì„ í˜•ì‹ì„ ì°¾ëŠ” ê²ƒì´ ë¬¸ì œì´ë‹¤

## 02-1 (Linear) Hypothesis
### ìˆ˜ì‹
$$H(x) = Wx + b (W : ê°€ì¤‘ì¹˜, b: bias)$$
ê°€ì¤‘ì¹˜ì™€ ì ˆí¸ì„ ì¡°ì ˆí•˜ì—¬ ì ì ˆí•œ ì„ í˜• ë°©ì •ì‹ì„ ì°¾ëŠ”ë‹¤.

## 02-02 Cost function(1) : square
 ğŸ’¡ ëª¨ë¸ê³¼ ì‹¤ì œ ë°ì´í„° ê°’ì˜ ì°¨ì´
 - How fit the line to our (training) data
 - ì œê³±
$$(H(x) - y )^{2} = loss function $$
ì´ë•Œ $y$ ëŠ” ì˜ˆì¸¡ ê°’
### ìˆ˜ì‹
$$cost = {1 \over m} \Sigma_{i=1}^m (H(x^{(i)} - y^{(i)})^2$$


# 03 How to minimize cost
- cost functionì˜ í˜•íƒœë¥¼ ë³´ë©´ -> $W$ ì— ëŒ€í•œ 2ì°¨ ë°©ì •ì‹ì˜ í˜•íƒœì´ë‹¤. -> ë”°ë¼ì„œ ê°’ì´ costì˜ ê°’ì´ ìµœì†Œê°€ ë˜ëŠ” $W$ ê°’ì„ ì°¾ì•„ì•¼ í•œë‹¤.
- ğŸ”¥ ìµœì†Œê°’ì„ ì°¾ëŠ” ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì´ ì¡´ì¬ í•œë‹¤. ì´ì¤‘ì—ì„œ ìš°ë¦¬ëŠ” `ê²½ì‚¬í•˜ê°•ë²•(gradiednt descent algorithm)` ì— ëŒ€í•´ í•™ìŠµí•œë‹¤. 

## 03-1 ê²½ì‚¬í•˜ê°•ë²•
- ì‘ë™ê¸°ì‘
1. start with initial guesses
 - start at 0,0 (or any other value)
 - Keeping changin **W** and **b** a little bit to try and reduce $cost(W, b)$
2. $cost(W, b)$ê°€ ê°ì†Œí•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ê° íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•œë‹¤.
3. ë”°ë¼ì„œ ìˆ˜ë ´í•˜ëŠ” `local minimum`ì„ ì°¾ëŠ”ë‹¤.

### ìˆ˜ì‹
$$W := W - \alpha {\partial \over \partial{W} } cost(W)$$
$\alpha$ : learning rate


# 05 Binary classification
ğŸ’¡ ëŒ€í‘œì  : ìŠ¤íŒ¸/ í˜ì´ìŠ¤ë¶ì˜ show or hide/ ì‹ ìš©ì¹´ë“œ ì´ìƒ ê±°ë˜ ê°ì§€
```
ì‹œê°„ì— ëŒ€í•œ íŒ¨ë…¼íŒ¨ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤ê³  ê°€ì •í•´ë³´ì
ìì—°ìŠ¤ëŸ½ê²Œ... ìƒê°í•˜ë©´ ê³µë¶€ì‹œê°„ì´ ëŠ˜ ìˆ˜ë¡ íŒ¨ìŠ¤ì¼ í™•ë¥ ì´ ë†’ë‹¤.
ë‹¤ë§Œ, ì´ì— ëŒ€í•œ H ë¥¼ ì„ í˜•ë°©ì •ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ê²Œ ë˜ë©´ pass = 1 ì¸ë° ê·¸ ì˜ˆì¸¡ê°’ì´ ë„ˆë¬´ ë†’ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ë¬¸ì œê°€ ì¡´ì¬í•œë‹¤.
ê·¸ë ‡ë‹¤ê³  ê¸°ìš¸ê¸°ë¥¼ ë‚®ì¶”ë©´ pass/ non passì˜ ì„ê³„ì ì´ ë‹¬ë¼ì ¸ ë¬¸ì œê°€ ëœë‹¤.

-> ë”°ë¼ì„œ ì´ëŸ¬í•œ ì„ í˜•í•¨ìˆ˜ë¥¼ ë¹„ì„ í˜•í•¨ìˆ˜ë¡œ ë§Œë“¤ì–´ ì‘ë™í•˜ê²Œ í•˜ëŠ” ê¸°ì‘ì´ activate function ì´ë‹¤
```

## 05-1 activate function(1) : Sigmoid
- We know Y is `0` or `1` 
 - $H(x) = Wx + b$
- Hypothesis can give calues large than 1 or less than 0

### ìˆ˜ì‹
$$z = Wx + b$$
$$H(x) = g(z)$$
$$g(z) = {1 \over {1+e^{-z}}}$$

## 05-2 cost function(2) - log
$$cost(W) = {1 \over m} \Sigma c(H(x), y)$$

$$c(H(x), y) = \begin{cases}
-log(H(x)) & :y=1 \\
-log(1-H(x)) & :y=0
\end{cases} = -ylog(H(x)) - (1-y)log(1-H(x))$$


# 06 Softmax classification: Multinomial classification
- ğŸ’¡ ì´ëŠ” ì•ì„œ ë°°ì› ë˜ binarayë¥¼ ì—¬ëŸ¬ê°œë¥¼ ì‘ìš©í•œë‹¤ê³  ìƒê°í•˜ë©´ëœë‹¤.<\br> ë§Œì•½ grade ê°€ A,B,C ê°€ ìˆë‹¤ë©´
 - A or not
 - B or not
 - C or not
<p align='center'><img src="https://user-images.githubusercontent.com/118495946/214567224-a1c7669d-4611-4e1c-ad69-5975f8ec3f01.png" width="600" height="300"/></p>
- ê° ê·¸ë¦¼ê³¼ ê°™ì´ 3ê°œì˜ **Sigmoid** ë¥¼ ì´ìš©í•˜ì—¬ 0~1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë³€í™˜ì‹œí‚¨ë‹¤.


## 06-1 activate funciton(2) Softmax
- ìœ„ ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê°’ì„ ëˆ„êµ¬ë¥¼ ì˜ˆìƒí• ì§€ì— ëŒ€í•´ì„œëŠ” ìƒˆë¡œìš´ ë°©ì‹ì¸  `soft max` ë¥¼ ì‚¬ìš©í•´ ë³´ì. ì´ëŠ” í™•ë¥ ì ì¸ ë°©ë²•ë¡ ì„ ì´ìš©í•˜ì—¬ ê°€ì¥ ê°’ì´ í° ê²ƒì„ íƒ€ê²Ÿìœ¼ë¡œ ì„ ì •í•˜ëŠ” ë°©ì‹ì´ë‹¤
- one-hot encoding ê³¼ ì„¸íŠ¸ë¡œ ê¸°ì–µí•˜ì 

### ìˆ˜ì‹
$$S(y_{i}) = {e^{y_{i}} \over {\Sigma e^{y_{i}}}}$$

## 06-2 cost function(3) - cross-entropy cost function
### ìˆ˜ì‹
$$D(S, L) = - \Sigma_{i} L_{i} log(S_{i})$$ 
$S(y)$ ì˜ˆì¸¡ ê°’ $L$ ì‹¤ì œ ê°’ -> ì•ì„œ ë°°ìš´ cost function ê³¼ ì‹¤ì§ˆì  ë™ì¼

# 07 Learning rate, data preprocessing & overfitting


