

# 01 머신러닝의 개요
 - 💡 supervised : learning with labeled examples (training set) `주로 배울내용`
<p align='center'><img src="https://user-images.githubusercontent.com/118495946/214555562-b7df5d97-b9e8-471a-9989-0a9dc50ae727.jpeg" width="600" height="300"/></p>
 -  💡 Unsupervised learning : un-labeled data
  - Google news grouping
  - Word clustering

## 01-1 types of supervised learning
1. 💡 `regression` : prediction final exam score based on time spent (연속적인 값)
2. 💡 `binart calssification` : pass/non-pass based on time spent ( 2개의 값)
3. 💡 `multi-label classification` : Letter grade based on time spent ( 불연속적인 여러개의 값)

# 02 선형 회귀
💡 대표적인 예시로 : prediction final exam score based on time spent 가 있다고 하자 <\br>
각 `training dataset` 을 체크하고 이와 가장 잘 맞는 선형식을 찾는 것이 문제이다

## 02-1 (Linear) Hypothesis
### 수식
$$H(x) = Wx + b (W : 가중치, b: bias)$$
가중치와 절편을 조절하여 적절한 선형 방정식을 찾는다.

## 02-02 Cost function(1) : square
 💡 모델과 실제 데이터 값의 차이
 - How fit the line to our (training) data
 - 제곱
$$(H(x) - y )^{2} = loss function $$
이때 $y$ 는 예측 값
### 수식
$$cost = {1 \over m} \Sigma_{i=1}^m (H(x^{(i)} - y^{(i)})^2$$


# 03 How to minimize cost
- cost function의 형태를 보면 -> $W$ 에 대한 2차 방정식의 형태이다. -> 따라서 값이 cost의 값이 최소가 되는 $W$ 값을 찾아야 한다.
- 🔥 최소값을 찾는 다양한 알고리즘이 존재 한다. 이중에서 우리는 `경사하강법(gradiednt descent algorithm)` 에 대해 학습한다. 

## 03-1 경사하강법
- 작동기작
1. start with initial guesses
 - start at 0,0 (or any other value)
 - Keeping changin **W** and **b** a little bit to try and reduce $cost(W, b)$
2. $cost(W, b)$가 감소하는 방향으로 각 파라미터를 변경한다.
3. 따라서 수렴하는 `local minimum`을 찾는다.

### 수식
$$W := W - \alpha {\partial \over \partial{W} } cost(W)$$
$\alpha$ : learning rate


# 05 Binary classification
💡 대표적 : 스팸/ 페이스북의 show or hide/ 신용카드 이상 거래 감지
```
시간에 대한 패논패 여부를 나타낸다고 가정해보자
자연스럽게... 생각하면 공부시간이 늘 수록 패스일 확률이 높다.
다만, 이에 대한 H 를 선형방정식으로 나타내게 되면 pass = 1 인데 그 예측값이 너무 높게 나타나는 문제가 존재한다.
그렇다고 기울기를 낮추면 pass/ non pass의 임계점이 달라져 문제가 된다.

-> 따라서 이러한 선형함수를 비선형함수로 만들어 작동하게 하는 기작이 activate function 이다
```

## 05-1 activate function(1) : Sigmoid
- We know Y is `0` or `1` 
 - $H(x) = Wx + b$
- Hypothesis can give calues large than 1 or less than 0

### 수식
$$z = Wx + b$$
$$H(x) = g(z)$$
$$g(z) = {1 \over {1+e^{-z}}}$$

## 05-2 새로운 cost function
$$cost(W) = {1 \over m} \Sigma c(H(x), y)$$
$$c(H(x), y) = \begin{cases}
-log(H(x)), & :y=1 \\
-log(1-H(x)), & :y=0
\end{cases}$$
